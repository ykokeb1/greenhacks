{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FEATURES_FINAL.csv\")\n",
    "X_df = df.iloc[:, 3:]\n",
    "y_df = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_df = std.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Validation Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_df, y_df, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(algorithm):\n",
    "    '''\n",
    "        function that takes in a classifier model and returns metrics based on prediciton of validation data \n",
    "        parameters:\n",
    "                    algorithm is classifier with paramaters (e.g, sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None))\n",
    "        returns: \n",
    "                    accuracy, macro F1, and weighted F1 scores\n",
    "    ''' \n",
    "    \n",
    "    # Fit the Model\n",
    "    alg = algorithm \n",
    "    alg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict y_val from X_val\n",
    "    y_val_pred = alg.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy, macro F1, and weighted F1 metrics\n",
    "    acc = sklearn.metrics.accuracy_score(y_val, y_val_pred, normalize=True, sample_weight=None)\n",
    "    mf1 = sklearn.metrics.f1_score(y_val, y_val_pred, average='macro') \n",
    "    wf1 = sklearn.metrics.f1_score(y_val, y_val_pred, average='weighted') \n",
    "    \n",
    "    return acc, mf1, wf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forest  0.7058823529411765 0.38271604938271603 0.681917211328976\n",
      "\n",
      " K Nearest Neighbors 0.7058823529411765 0.27586206896551724 0.6328600405679513\n",
      "\n",
      " Decision Tree 0.4117647058823529 0.40404040404040403 0.45632798573975053\n"
     ]
    }
   ],
   "source": [
    "# Random Forest:\n",
    "from sklearn import ensemble \n",
    "rf = sklearn.ensemble.RandomForestClassifier() # make 10, bc takes long?\n",
    "rf_acc, rf_macro, rf_wei = model_eval(rf) \n",
    "print(\"\\n Random Forest \", rf_acc, rf_macro, rf_wei)\n",
    "\n",
    "# K Nearest Neighbors \n",
    "from sklearn import neighbors \n",
    "knn = sklearn.neighbors.KNeighborsClassifier() \n",
    "knn_acc, knn_macro, knn_wei = model_eval(knn) \n",
    "print(\"\\n K Nearest Neighbors\", knn_acc, knn_macro, knn_wei)\n",
    "\n",
    "# Decision Tree \n",
    "from sklearn import tree \n",
    "dt = sklearn.tree.DecisionTreeClassifier() \n",
    "dt_acc, dt_macro, dt_wei = model_eval(dt) \n",
    "print(\"\\n Decision Tree\", dt_acc, dt_macro, dt_wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNING HYPERPARAMETERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "{'n_estimators': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 10, 'bootstrap': True}\n",
      "RandomForestClassifier(max_depth=10, n_estimators=10)\n",
      "\n",
      " Hypertuned Random Forest  0.47058823529411764 0.26956521739130435 0.50076726342711\n"
     ]
    }
   ],
   "source": [
    "# Random Forest -- Tuning Hyperparameters \n",
    "from sklearn import ensemble\n",
    "rf_new = sklearn.ensemble.RandomForestClassifier(n_estimators = 5)\n",
    "    \n",
    "# Parameters to Manipulate  \n",
    "    # Number of Trees\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 5)]\n",
    "    # Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "    # Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create parameter distributions\n",
    "rf_param = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Random Forest -- Random Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_rand = RandomizedSearchCV(estimator = rf_new, param_distributions = rf_param, cv = 3, verbose = 2, n_jobs = 4)\n",
    "rf_rand.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest -- Random Search Results \n",
    "print(rf_rand.best_params_)\n",
    "print(rf_rand.best_estimator_)\n",
    "\n",
    "# Random Forest -- New Metrics \n",
    "rf_new_acc, rf_new_macro, rf_new_wei = model_eval(rf_rand.best_estimator_)\n",
    "print(\"\\n Hypertuned Random Forest \", rf_new_acc, rf_new_macro, rf_new_wei)\n",
    "\n",
    "# Create \"Final\" RF with Best Estimator \n",
    "rf_final = rf_rand.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 500, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 680, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 23, n_neighbors = 28\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 500, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 680, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 23, n_neighbors = 28\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 500, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 680, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 23, n_neighbors = 28\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 500, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 680, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 23, n_neighbors = 28\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 500, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 680, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 24, n_neighbors = 28\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.51333333 0.65333333        nan 0.62       0.65333333 0.62\n",
      " 0.62       0.65333333 0.65333333 0.62      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 2, 'n_neighbors': 19, 'leaf_size': 13}\n",
      "KNeighborsClassifier(leaf_size=13, n_neighbors=19)\n",
      "\n",
      " Hypertuned KNN  0.7647058823529411 0.28888888888888886 0.6627450980392157\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors \n",
    "from sklearn import neighbors \n",
    "knn_new = sklearn.neighbors.KNeighborsClassifier()\n",
    "\n",
    "# Create parameter distributions \n",
    "knn_param = {'leaf_size': list(range(1,50)), \n",
    "             'n_neighbors': list(range(1,30)), \n",
    "             'p': [1,2]}\n",
    "\n",
    "# KNN -- Random Search \n",
    "knn_rand = RandomizedSearchCV(estimator = knn_new, param_distributions = knn_param, cv = 5)\n",
    "knn_rand.fit(X_train, y_train)\n",
    "\n",
    "# KNN -- Random Search Results \n",
    "print(knn_rand.best_params_)\n",
    "print(knn_rand.best_estimator_)\n",
    "\n",
    "# KNN -- New Metrics \n",
    "knn_new_acc, knn_new_macro, knn_new_wei = model_eval(knn_rand.best_estimator_)\n",
    "print(\"\\n Hypertuned KNN \", knn_new_acc, knn_new_macro, knn_new_wei)\n",
    "\n",
    "# Create \"Final\" KNN with Best Estimator \n",
    "knn_final = knn_rand.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 3, 'max_depth': 3, 'criterion': 'gini'}\n",
      "DecisionTreeClassifier(max_depth=3, min_samples_leaf=3)\n",
      "\n",
      " Hypertuned DT  0.4117647058823529 0.44949494949494956 0.45365418894830656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prsah\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree \n",
    "from sklearn import tree \n",
    "dt_new = sklearn.tree.DecisionTreeClassifier()\n",
    "\n",
    "# Create parameter distributions \n",
    "dt_param = {\"max_depth\": [3, None],\n",
    "            \"min_samples_leaf\": list(range(1,9)),\n",
    "            \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# DT -- Random Search \n",
    "dt_rand = RandomizedSearchCV(estimator = dt_new, param_distributions = dt_param)\n",
    "dt_rand.fit(X_train, y_train)\n",
    "\n",
    "# DT -- Random Search Results \n",
    "print(dt_rand.best_params_)\n",
    "print(dt_rand.best_estimator_)\n",
    "\n",
    "# DT -- New Metrics \n",
    "dt_new_acc, dt_new_macro, dt_new_wei = model_eval(dt_rand.best_estimator_)\n",
    "print(\"\\n Hypertuned DT \",dt_new_acc, dt_new_macro, dt_new_wei)\n",
    "\n",
    "# Create \"Final\" DT with Best Estimator \n",
    "dt_final = dt_rand.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Algorithm Name      Accuracy           Macro F1           Weighted F1\n",
      "\n",
      " Hypertuned Random Forest  0.7058823529411765 0.3772893772893773 0.6974789915966387\n",
      "\n",
      " Hypertuned KNN  0.7647058823529411 0.28888888888888886 0.6627450980392157\n",
      "\n",
      " Hypertuned DT  0.4117647058823529 0.44949494949494956 0.45365418894830656\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Algorithm Name     \", \"Accuracy\", \"          Macro F1\", \"          Weighted F1\")\n",
    "\n",
    "rf_fin_acc, rf_fin_macro, rf_fin_wei = model_eval(rf_final)\n",
    "knn_fin_acc, knn_fin_macro, knn_fin_wei = model_eval(knn_final)\n",
    "dt_fin_acc, dt_fin_macro, dt_fin_wei = model_eval(dt_final)\n",
    "\n",
    "print(\"\\n Hypertuned Random Forest \", rf_fin_acc, rf_fin_macro, rf_fin_wei)\n",
    "print(\"\\n Hypertuned KNN \", knn_fin_acc, knn_fin_macro, knn_fin_wei)\n",
    "print(\"\\n Hypertuned DT \",dt_fin_acc, dt_fin_macro, dt_fin_wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52966672  0.91766294 -0.1435222  -0.85599914 -0.83473955 -0.82736328\n",
      "  -0.84741546 -0.76807383]\n",
      " [-0.13381059  0.91766294 -0.14290671 -0.21072151 -0.51604375 -0.10144859\n",
      "  -0.58300899 -0.63644819]\n",
      " [-0.64837366 -1.3764944  -0.1433753  -0.65465197 -0.72794412  0.52197848\n",
      "  -0.78314708 -0.71776529]\n",
      " [ 0.02066147 -0.22941573 -0.14272447 -0.47910316 -0.17032191 -0.38482452\n",
      "  -0.12310818  0.31657899]]\n",
      "[1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Actual Predictions?: \n",
    "print(X_test)\n",
    "final_predictions = dt_final.predict(X_test)\n",
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
